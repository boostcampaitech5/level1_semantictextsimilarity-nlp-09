{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L4MxO4-RgNRR"
   },
   "source": [
    "# NLP 기초대회 미션 1 - 탐색적 데이터 분석(EDA)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCnVIiHogMv_"
   },
   "source": [
    "**데이터분석(EDA) 미션 : KLUE STS 데이터셋에 대한 간단한 분석을 수행합니다**\n",
    "\n",
    "**미션 개요**\n",
    "- 자연어처리 EDA 배우기\n",
    "\n",
    "**실습 배경 및 목적**\n",
    "- pandas를 활용한 데이터 분석\n",
    "- 데이터 분석 결과 시각화\n",
    "- 자연어처리에서의 데이터 분석\n",
    "\n",
    "**데이터셋(https://klue-benchmark.com/tasks/67/overview/description)**\n",
    "- KLUE의 Semantic Textual Similarity(STS) 데이터셋\n",
    "- 입력 : 두 문장\n",
    "- 출력 : 두 문장의 유사도\n",
    "- 학습 데이터 : 11,668개\n",
    "- 검증 데이터 : 519개(평가 데이터가 비공개이므로 학습에서 평가데이터로 활용함)\n",
    "- 평가 데이터 : 1,037개(비공개)\n",
    "- License : <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\">Creative Commons Attribution-ShareAlike 4.0 International License</a>.\n",
    "\n",
    "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-sa/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-sa/4.0/88x31.png\" /></a><br />\n",
    "\n",
    "**모델**\n",
    "- [klue/roberta-small](https://huggingface.co/klue/roberta-small) 모델과 토크나이저 활용"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "대회를 시작하기에 앞서, 우리가 어떤 데이터로, 어떤 입출력을 설계해야할지 구상하는 것이 중요합니다. 좋은 설계는 좋은 성능의 근간이 될 수 있으니까요. 특히, 자연어 데이터에 대해 아직 익숙하지 않을 수 있는 여러분을 위해 데이터 탐색을 함께 진행하겠습니다. 이번 미션에서 우리는 자연어 데이터에 대해 사용할 수 있는 탐색적 데이터 분석(Exploratory Data Analysis, EDA) 과정을 배웁니다.\n",
    "\n",
    "EDA를 위해, 우리는 대회와 같은 문제에 대해 접근하고 있는 KLUE-STS 데이터를 활용합니다. Pandas 라이브러리를 이용해 EDA를 더욱 쉽게 할 수 있는 방법을 배웁니다. 처음 자연어 데이터를 다루는 분이라도 예제를 차분히 따라하시다보면 대회에서 데이터셋 분석을 더 쉽게 하실 수 있을 것입니다!\n",
    "\n",
    "\n",
    "\n",
    "1. KLUE-STS 데이터셋에 대해 분석을 진행합니다.\n",
    "\n",
    "STS 데이터셋의 구조를 분석하고, 이해하는 것을 목표로 EDA를 진행합니다. 분석 결과에 대한 이해를 바탕으로 데이터에 포함된 정보 중 이용할 속성을 선정하고, 이를 어떻게 활용할지에 대한 딥러닝 훈련 방식을 설계하는 토대를 마련합니다.\n",
    "\n",
    "2. Pandas를 활용한 자연어 데이터의 특성을 시각화한다.\n",
    "\n",
    "자연어 데이터는 시각화가 매우 어려운 데이터에 속합니다. 시각화가 가능하고 실제로 훈련 방식을 설계하는 데에 필요할 자연어 데이터의 특성이 무엇이 있을까를 살피며, 이에 대해 시각화 방법을 생각해봅니다. Pandas의 다양한 기능을 활용하여 자연어 데이터를 그래프, 표 등으로 시각화합니다. 아래의 Pandas의 Dataframe에 대한 문서와 그중에서도 그래프를 바로 생성하는 plot에 대한 문서를 참고하세요!\n",
    "\n",
    "Pandas Dataframe\n",
    "\n",
    "Pandas Plotting\n",
    "\n",
    "EDA는 데이터 분석에서 중요한 단계 중 하나인 \"탐색적 데이터 분석(Exploratory Data Analysis)\"의 약어입니다. 데이터 분석을 시작하기 전에 데이터를 탐색하고 이해하는 과정으로, 데이터의 특성을 파악하고 데이터셋 내의 패턴이나 규칙을 발견하는 것을 목적으로 합니다.\n",
    "\n",
    "EDA는 데이터 분석 과정에서 매우 중요한 단계로, 데이터셋의 특성과 문제점을 파악하여 데이터 전처리나 모델링에 필요한 인사이트를 제공해줍니다. 또한, EDA 과정에서 발견된 통찰력을 토대로 더 나은 데이터 분석 방법을 찾을 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9j6Mt6hoCHF6"
   },
   "source": [
    "# 환경설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 7106,
     "status": "ok",
     "timestamp": 1679277822486,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "jHOl6EMssGSC",
    "outputId": "e49d7613-6966-4c45-a83c-4c0a79eaddb7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.27.4-py3-none-any.whl (6.8 MB)\n",
      "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
      "  Downloading tokenizers-0.13.3-cp38-cp38-win_amd64.whl (3.5 MB)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from transformers) (2.25.1)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from transformers) (20.9)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers) (3.0.12)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers) (4.59.0)\n",
      "Collecting huggingface-hub<1.0,>=0.11.0\n",
      "  Downloading huggingface_hub-0.13.4-py3-none-any.whl (200 kB)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (2021.4.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (1.20.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (3.7.4.3)\n",
      "Requirement already satisfied: pyparsing>=2.0.2 in c:\\anaconda3\\lib\\site-packages (from packaging>=20.0->transformers) (2.4.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (1.26.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2022.12.7)\n",
      "Requirement already satisfied: chardet<5,>=3.0.2 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (4.0.0)\n",
      "Requirement already satisfied: idna<3,>=2.5 in c:\\anaconda3\\lib\\site-packages (from requests->transformers) (2.10)\n",
      "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
      "Successfully installed huggingface-hub-0.13.4 tokenizers-0.13.3 transformers-4.27.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: You are using pip version 21.2.2; however, version 23.0.1 is available.\n",
      "You should consider upgrading via the 'c:\\anaconda3\\python.exe -m pip install --upgrade pip' command.\n",
      "Cloning into 'KLUE'...\n",
      "Updating files:  55% (16/29)\n",
      "Updating files:  58% (17/29)\n",
      "Updating files:  62% (18/29)\n",
      "Updating files:  65% (19/29)\n",
      "Updating files:  68% (20/29)\n",
      "Updating files:  72% (21/29)\n",
      "Updating files:  75% (22/29)\n",
      "Updating files:  79% (23/29)\n",
      "Updating files:  82% (24/29)\n",
      "Updating files:  86% (25/29)\n",
      "Updating files:  89% (26/29)\n",
      "Updating files:  93% (27/29)\n",
      "Updating files:  96% (28/29)\n",
      "Updating files: 100% (29/29)\n",
      "Updating files: 100% (29/29), done.\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers\n",
    "!git clone https://github.com/KLUE-benchmark/KLUE.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 411,
     "status": "ok",
     "timestamp": 1679277709397,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "jkOzYrEos0lJ",
    "tags": []
   },
   "outputs": [],
   "source": [
    "#import json # used as a lightweight data interchange format.\n",
    "\n",
    "import pandas as pd # 판다스 import\n",
    "from matplotlib import pyplot as plt # matplotlib에서 pypplot 다운로드\n",
    " \n",
    "from tqdm import tqdm # tqdm import\n",
    "#from transformers import AutoTokenizer # hugging face에서 AutoTokenizer불러옴"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hugging Face에서 제공하는 AutoTokenizer는 사용자가 입력한 토크나이저 이름에 따라 자동으로 해당 토크나이저를 선택하여 로드하는 기능을 제공하는 클래스입니다.\n",
    "\n",
    "자연어 처리에서 토크나이저는 입력 텍스트를 토큰 단위로 분리하는 작업을 말합니다. 예를 들어, 문장을 단어나 문장부호로 나누는 작업이 이에 해당합니다. Hugging Face에서는 다양한 언어와 모델에 대한 토크나이저를 제공하고 있으며, AutoTokenizer는 이러한 토크나이저들 중에서 자동으로 선택해주는 기능을 제공합니다.\n",
    "\n",
    "예를 들어, AutoTokenizer.from_pretrained() 함수를 사용하여 모델 이름을 지정하면 해당 모델의 기본 토크나이저를 로드할 수 있습니다. 또한, 특정 토크나이저를 사용하고자 하는 경우, 토크나이저 이름을 직접 지정하여 로드할 수도 있습니다. 이를 통해 사용자는 토크나이저를 선택하는 번거로움을 덜고 간편하게 모델을 사용할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OlMFdV3NBJuf"
   },
   "source": [
    "# 사용 함수 정의"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 399,
     "status": "ok",
     "timestamp": 1679277868167,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "c6hKmC64s-RI"
   },
   "outputs": [],
   "source": [
    "# json 데이터를 pandas 형태로 읽어옵니다\n",
    "def read_json(data_type):\n",
    "    with open(f'./KLUE/klue_benchmark/klue-sts-v1.1/klue-sts-v1.1_{data_type}.json', 'r', encoding='utf-8') as file:\n",
    "        json_data = json.load(file)\n",
    "\n",
    "    data = []\n",
    "    for item in tqdm(json_data, desc='read json', total=len(json_data)):\n",
    "        data.append([item['source'], item['sentence1'], item['sentence2'], item['labels']['label'], item['labels']['binary-label']])\n",
    "\n",
    "    df = pd.DataFrame(data, columns=['source', 'sentence1', 'sentence2', 'label', 'binary_label'])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1679277739164,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "A-T86UM9s-L3"
   },
   "outputs": [],
   "source": [
    "# 입력의 두 문장을 토크나이징하여 길이와 unk 토큰의 개수를 분석합니다\n",
    "def tokenizing(df):\n",
    "    tokenizer = AutoTokenizer.from_pretrained('klue/roberta-small') # 사전학습된 KLUE의 robera-small tokenizer를 불러옴\n",
    "                                                                    # 모델 : [klue/roberta-small](https://huggingface.co/klue/roberta-small)모델과 토크나이저 활용\n",
    "\n",
    "    sentence1_len = []\n",
    "    sentence2_len = []\n",
    "    sentence1_unk = []\n",
    "    sentence2_unk = []\n",
    "    for i, item in df.iterrows():\n",
    "        sentence1 = tokenizer(item['sentence1'])['input_ids']\n",
    "        sentence2 = tokenizer(item['sentence2'])['input_ids']\n",
    "\n",
    "        sentence1_len.append(len(sentence1)) # 토큰화된 문장의 길이\n",
    "        sentence2_len.append(len(sentence2)) # 토큰화된 문장의 길이\n",
    "\n",
    "        sentence1_unk.append(sentence1.count(tokenizer.unk_token_id)) # 'UNK' 토큰의 개수 추가\n",
    "        sentence2_unk.append(sentence2.count(tokenizer.unk_token_id)) # 'UNK' 토큰의 개수 추가\n",
    "\n",
    "    tokenized_df = pd.DataFrame([sentence1_len, sentence2_len, sentence1_unk, sentence2_unk]).transpose()\n",
    "    tokenized_df.columns = ['1_len', '2_len', '1_unk', '2_unk']\n",
    "    print(tokenized_df.describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 코드는 해당 데이터셋에서 문장의 길이와 'UNK' 토큰의 등장 빈도를 분석하여, 데이터셋의 특성을 파악하는 과정으로, 데이터 전처리나 모델링의 방향성을 제시해줄 수 있습니다. 이를 통해 모델 학습 시 문제를 예측하고 방지할 수 있으며, 모델의 성능을 향상시킬 수 있음."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 545,
     "status": "ok",
     "timestamp": 1679277741767,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "5RH28Zwx68Qp"
   },
   "outputs": [],
   "source": [
    "# label의 분포를 box plot으로 그립니다\n",
    "def draw_box_plot(df):\n",
    "    label_list = [row['label'] for i, row in df.iterrows()]\n",
    "\n",
    "    plt.boxplot(label_list)\n",
    "    plt.title(\"Boxplot for target label\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 9,
     "status": "ok",
     "timestamp": 1679277743510,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "fjmAGbxBtDc3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# pandas 데이터를 가지고 그래프를 그리는 함수입니다\n",
    "def draw_graph(df):\n",
    "    source_binary = {source:[0, 0] for source in sorted(df['source'].unique())}\n",
    "    for i, item in df.iterrows():\n",
    "        source_binary[item['source']][item['binary_label']] += 1\n",
    "    source_binary_df = pd.DataFrame(source_binary).transpose()\n",
    "\n",
    "    # source의 분포\n",
    "    df['source'].value_counts().plot(kind='bar', rot=20, figsize=(8, 6))\n",
    "    plt.show()\n",
    "    # source별 binary label의 분포\n",
    "    source_binary_df.plot(kind='bar', rot=20, figsize=(8, 6))\n",
    "    plt.show()\n",
    "    # label의 분포\n",
    "    df['label'].plot(kind='hist', rot=20, figsize=(8, 6))\n",
    "    plt.show()\n",
    "    # binary label의 분포\n",
    "    df['binary_label'].value_counts().plot(kind='bar', rot=20, figsize=(8, 6))\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MMVhCyUTCEpi"
   },
   "source": [
    "# 데이터 분석"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "        self.target_columns = ['label']\n",
    "        self.delete_columns = ['id']\n",
    "        self.text_columns = ['sentence_1', 'sentence_2']\n",
    "\n",
    "def preprocessing(data):\n",
    "    # 안쓰는 컬럼을 삭제합니다.\n",
    "    data = data.drop(columns='id')\n",
    "\n",
    "    # 타겟 데이터가 없으면 빈 배열을 리턴합니다.\n",
    "    try:\n",
    "        targets = data['label'].values.tolist()\n",
    "    except:\n",
    "        targets = []\n",
    "    # 텍스트 데이터를 전처리합니다.\n",
    "    inputs = self.tokenizing(data)\n",
    "\n",
    "    return inputs, targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 23,
     "status": "ok",
     "timestamp": 1679277871978,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "DcU8pAictFpl",
    "outputId": "fd5fa952-59af-48ae-c247-440731d428b2"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "read json: 100%|██████████| 11668/11668 [00:00<00:00, 865199.40it/s]\n",
      "read json: 100%|██████████| 519/519 [00:00<00:00, 518790.22it/s]\n"
     ]
    }
   ],
   "source": [
    "# 데이터 읽기\n",
    "train_df = read_json('train')\n",
    "dev_df = read_json('dev')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../data/train.csv')\n",
    "dev_df = pd.read_csv('../data/dev.csv')\n",
    "\n",
    "data_df = pd.concat([train_df, dev_df], axis=0)\n",
    "#train_df = pd.DataFrame(data)\n",
    "#df = pd.DataFrame(data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>source</th>\n",
       "      <th>sentence_1</th>\n",
       "      <th>sentence_2</th>\n",
       "      <th>label</th>\n",
       "      <th>binary-label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boostcamp-sts-v1-train-000</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~</td>\n",
       "      <td>반전도 있고,사랑도 있고재미도있네요.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>boostcamp-sts-v1-train-001</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>앗 제가 접근권한이 없다고 뜹니다;;</td>\n",
       "      <td>오, 액세스 권한이 없다고 합니다.</td>\n",
       "      <td>4.2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>boostcamp-sts-v1-train-002</td>\n",
       "      <td>petition-sampled</td>\n",
       "      <td>주택청약조건 변경해주세요.</td>\n",
       "      <td>주택청약 무주택기준 변경해주세요.</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>boostcamp-sts-v1-train-003</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>입사후 처음 대면으로 만나 반가웠습니다.</td>\n",
       "      <td>화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>boostcamp-sts-v1-train-004</td>\n",
       "      <td>slack-sampled</td>\n",
       "      <td>뿌듯뿌듯 하네요!!</td>\n",
       "      <td>꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>545</th>\n",
       "      <td>boostcamp-sts-v1-dev-545</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>사회적 이슈를 다루고 있는 가슴 찡한 드라마네요,,,</td>\n",
       "      <td>정말 가슴을 따뜻하게 한 좋은 드라마...</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>546</th>\n",
       "      <td>boostcamp-sts-v1-dev-546</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>(비타민 먹는 장면)</td>\n",
       "      <td>(비타민을 먹는 장면)</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>547</th>\n",
       "      <td>boostcamp-sts-v1-dev-547</td>\n",
       "      <td>nsmc-sampled</td>\n",
       "      <td>내용이 뭔 내용인지도 모르겠음</td>\n",
       "      <td>무슨의미로 만들었는지 모르겠음..</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>548</th>\n",
       "      <td>boostcamp-sts-v1-dev-548</td>\n",
       "      <td>slack-rtt</td>\n",
       "      <td>(예: 주말에는 개인캘린더만, 업무시간에는 업무 캘린더만 보기)</td>\n",
       "      <td>(예: 주말에는 개인캘린더만 보고, 업무시간에는 업무캘린더만 보기)</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>549</th>\n",
       "      <td>boostcamp-sts-v1-dev-549</td>\n",
       "      <td>nsmc-rtt</td>\n",
       "      <td>다소 허접한 영상도 군데군데 있음.</td>\n",
       "      <td>엉뚱한 영상도 몇 개 있습니다.</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9874 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             id            source  \\\n",
       "0    boostcamp-sts-v1-train-000      nsmc-sampled   \n",
       "1    boostcamp-sts-v1-train-001         slack-rtt   \n",
       "2    boostcamp-sts-v1-train-002  petition-sampled   \n",
       "3    boostcamp-sts-v1-train-003     slack-sampled   \n",
       "4    boostcamp-sts-v1-train-004     slack-sampled   \n",
       "..                          ...               ...   \n",
       "545    boostcamp-sts-v1-dev-545      nsmc-sampled   \n",
       "546    boostcamp-sts-v1-dev-546         slack-rtt   \n",
       "547    boostcamp-sts-v1-dev-547      nsmc-sampled   \n",
       "548    boostcamp-sts-v1-dev-548         slack-rtt   \n",
       "549    boostcamp-sts-v1-dev-549          nsmc-rtt   \n",
       "\n",
       "                                 sentence_1  \\\n",
       "0    스릴도있고 반전도 있고 여느 한국영화 쓰레기들하고는 차원이 다르네요~   \n",
       "1                      앗 제가 접근권한이 없다고 뜹니다;;   \n",
       "2                            주택청약조건 변경해주세요.   \n",
       "3                    입사후 처음 대면으로 만나 반가웠습니다.   \n",
       "4                                뿌듯뿌듯 하네요!!   \n",
       "..                                      ...   \n",
       "545           사회적 이슈를 다루고 있는 가슴 찡한 드라마네요,,,   \n",
       "546                             (비타민 먹는 장면)   \n",
       "547                        내용이 뭔 내용인지도 모르겠음   \n",
       "548     (예: 주말에는 개인캘린더만, 업무시간에는 업무 캘린더만 보기)   \n",
       "549                     다소 허접한 영상도 군데군데 있음.   \n",
       "\n",
       "                                sentence_2  label  binary-label  \n",
       "0                     반전도 있고,사랑도 있고재미도있네요.    2.2           0.0  \n",
       "1                      오, 액세스 권한이 없다고 합니다.    4.2           1.0  \n",
       "2                       주택청약 무주택기준 변경해주세요.    2.4           0.0  \n",
       "3             화상으로만 보다가 리얼로 만나니 정말 반가웠습니다.    3.0           1.0  \n",
       "4                    꼬옥 실제로 한번 뵈어요 뿌뿌뿌~!~!    0.0           0.0  \n",
       "..                                     ...    ...           ...  \n",
       "545                정말 가슴을 따뜻하게 한 좋은 드라마...    2.0           0.0  \n",
       "546                           (비타민을 먹는 장면)    4.8           1.0  \n",
       "547                     무슨의미로 만들었는지 모르겠음..    2.4           0.0  \n",
       "548  (예: 주말에는 개인캘린더만 보고, 업무시간에는 업무캘린더만 보기)    5.0           1.0  \n",
       "549                      엉뚱한 영상도 몇 개 있습니다.    2.2           0.0  \n",
       "\n",
       "[9874 rows x 6 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.001, 1.0]    4163\n",
      "(1.0, 2.0]       1372\n",
      "(2.0, 3.0]       1294\n",
      "(3.0, 4.0]       2058\n",
      "(4.0, 5.0]        987\n",
      "Name: label, dtype: int64\n",
      "[4163, 1372, 1294, 2058, 987]\n",
      "[4163, 1372, 1294, 2058, 987]\n"
     ]
    }
   ],
   "source": [
    "train_path = '../data/train.csv'\n",
    "dev_path = '../data/dev.csv'\n",
    "\n",
    "def counts_data_range(train_path, dev_path):\n",
    "    train_df = pd.read_csv(train_path)\n",
    "    dev_df = pd.read_csv(dev_path)\n",
    "    data_df = pd.concat([train_df, dev_df], axis=0)\n",
    "\n",
    "    bins = pd.cut(data_df['label'], bins=[0, 1, 2, 3, 4, 5], include_lowest=True)\n",
    "\n",
    "    # 각 구간별로 개수를 세어 출력\n",
    "    counts = bins.value_counts(sort=False)\n",
    "    data_counts = [counts[i] for i in range(1, len(counts) + 1)]\n",
    "    #print(counts)\n",
    "    #print(data_counts)\n",
    "    return data_counts\n",
    "\n",
    "print(counts_data_range(train_path, dev_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([5, 2, 4, 4, 5])\n",
      "Weighted MSE: 9.569999694824219\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# 실제값과 예측값을 포함하는 텐서\n",
    "y_true = torch.tensor([5, 1.8, 3.1, 3.9, 4.6])\n",
    "y_pred = torch.tensor([2.5, 1.6, 2.9, 4.1, 2.8])\n",
    "\n",
    "# 각 실제값이 속한 구간의 인덱스를 계산\n",
    "bins = torch.tensor([5, 1, 2, 3, 4, 5]) #[0,1) [1,2) [2,3] [3,4) [4,5)\n",
    "bin_indices = torch.bucketize(y_true, bins)\n",
    "print(bin_indices)\n",
    "\n",
    "# 각 구간에 대한 가중치 계산\n",
    "weights = torch.tensor([1, 2, 3, 4, 5], dtype=torch.float32)\n",
    "bin_weights = weights[bin_indices - 1]\n",
    "\n",
    "# Weighted MSE 계산\n",
    "mse = torch.mean(bin_weights * (y_true - y_pred) ** 2)\n",
    "\n",
    "print(\"Weighted MSE:\", mse.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights : tensor([0.4744, 1.4394, 1.5261, 0.9596, 2.0008])\n",
      "bin_indices : tensor([3, 2, 4, 4, 3])\n",
      "bin_weights : tensor([1.5261, 1.4394, 0.9596, 0.9596, 1.5261])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "mse_loss() got an unexpected keyword argument 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[79], line 48\u001b[0m\n\u001b[1;32m     44\u001b[0m     loss \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmse_loss(y_pred, y_true, weight\u001b[38;5;241m=\u001b[39mbin_weights)\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mse\n\u001b[0;32m---> 48\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWeighted MSE:\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[43mweighted_MSE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_counts\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mitem())\n",
      "Cell \u001b[0;32mIn[79], line 44\u001b[0m, in \u001b[0;36mweighted_MSE\u001b[0;34m(data_counts, y_true, y_pred)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# Weighted MSE 계산\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;66;03m#mse = torch.mean(bin_weights * (y_true - y_pred) ** 2) \u001b[39;00m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mnn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctional\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mF\u001b[39;00m\n\u001b[0;32m---> 44\u001b[0m loss \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbin_weights\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mse\n",
      "\u001b[0;31mTypeError\u001b[0m: mse_loss() got an unexpected keyword argument 'weight'"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([2.4, 1.8, 3.1, 4.0, 2.9])\n",
    "y_pred = torch.tensor([2.5, 1.6, 2.9, 4.1, 2.8])\n",
    "\n",
    "#y_true = torch.tensor([2.4])\n",
    "#y_pred = torch.tensor([2.5])\n",
    "\n",
    "data_counts = [4163, 1372, 1294, 2058 ,987] # (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0], (4.0, 5.0]\n",
    "'''\n",
    "(-0.001, 1.0]    4163\n",
    "(3.0, 4.0]       2058\n",
    "(1.0, 2.0]       1372\n",
    "(2.0, 3.0]       1294\n",
    "(4.0, 5.0]        987\n",
    "'''\n",
    "\n",
    "# 각 구간별 가중치를 계산하는 함수\n",
    "def calculate_weights(bin_counts):\n",
    "    total_samples = sum(bin_counts)  # 전체 데이터 개수\n",
    "    bin_ratios = [count / total_samples for count in bin_counts]  # 각 구간별 비율\n",
    "    bin_weights = [1 / (ratio * len(bin_counts)) for ratio in bin_ratios]  # 각 구간별 가중치\n",
    "    return torch.tensor(bin_weights)\n",
    "\n",
    "def weighted_MSE(data_counts, y_true, y_pred):\n",
    "    \n",
    "    # weights 계산\n",
    "    weights = calculate_weights(data_counts) # 결과 예) tensor([0.4744, 1.4394, 1.5261, 0.9596, 2.0008])\n",
    "    print(f'weights : {weights}')\n",
    "\n",
    "    # 각 실제값(label)이 속한 구간의 인덱스를 계산\n",
    "    # bins = torch.tensor([0, 1, 2, 3, 4]) # (-0.001, 1.0], (1.0, 2.0], (2.0, 3.0], (3.0, 4.0], (4.0, 5.0]\n",
    "    # bin_indices = torch.bucketize(y_true, bins)\n",
    "    bin_indices = torch.bucketize(y_true, torch.tensor([0, 1, 2, 3, 4]))\n",
    "    print(f'bin_indices : {bin_indices}')\n",
    "    \n",
    "    # 각 label에 대한 weight 설정\n",
    "    bin_weights = weights[bin_indices - 1]\n",
    "    print(f'bin_weights : {bin_weights}')\n",
    "\n",
    "     Weighted MSE 계산\n",
    "    mse = torch.mean(bin_weights * (y_true - y_pred) ** 2) \n",
    "    \n",
    "    return mse\n",
    "\n",
    "print(\"Weighted MSE:\", weighted_MSE(data_counts, y_true, y_pred).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-04-19-01:45:54\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "import pytz\n",
    "\n",
    "kr_tz = pytz.timezone('Asia/Seoul')\n",
    "now = datetime.datetime.now(tz=kr_tz)\n",
    "folder_name = now.strftime('%Y-%m-%d-%H:%M:%S')\n",
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 2, 4, 4, 5])\n"
     ]
    }
   ],
   "source": [
    "y_true = torch.tensor([-0.00001, 1.8, 3.1, 4.0, 7.4])\n",
    "bins = torch.tensor([0, 1, 2, 3, 4])\n",
    "bin_indices = torch.bucketize(y_true, bins) # (0, 1], (1, 2], (2, 3], (3, 4], (4, 5]\n",
    "print(bin_indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2.0, 3.0]       106\n",
      "(1.0, 2.0]       104\n",
      "(3.0, 4.0]       103\n",
      "(-0.001, 1.0]     98\n",
      "(4.0, 5.0]        95\n",
      "Name: target_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일을 읽어들입니다.\n",
    "df = pd.read_csv('my_log/2023-04-18-09-55-16/wrong.csv')\n",
    "\n",
    "# 'target' column의 범위를 나누어서 해당 범위에 속하는 row의 개수를 카운트합니다.\n",
    "bins = [-0.001, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "labels = ['(-0.001, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]']\n",
    "df['target_range'] = pd.cut(df['target'], bins=bins, labels=labels)\n",
    "count = df['target_range'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1.0, 2.0]       30\n",
      "(2.0, 3.0]       26\n",
      "(3.0, 4.0]       17\n",
      "(-0.001, 1.0]    11\n",
      "(4.0, 5.0]        1\n",
      "Name: target_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# csv 파일을 읽어들입니다.\n",
    "df = pd.read_csv('my_log/2023-04-18-09-55-16/wrong.csv')\n",
    "\n",
    "# 'pred'와 'target' column의 차이가 1.0 이상인 데이터를 추출합니다.\n",
    "diff = abs(df['pred'] - df['target'])\n",
    "df = df[diff >= 1.0]\n",
    "\n",
    "# 'target' column의 범위를 나누어서 해당 범위에 속하는 row의 개수를 카운트합니다.\n",
    "bins = [-0.001, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "labels = ['(-0.001, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]']\n",
    "df['target_range'] = pd.cut(df['target'], bins=bins, labels=labels)\n",
    "count = df['target_range'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(-0.001, 1.0]    110\n",
      "(1.0, 2.0]       110\n",
      "(2.0, 3.0]       110\n",
      "(3.0, 4.0]       110\n",
      "(4.0, 5.0]       110\n",
      "Name: target_range, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# csv 파일을 읽어들입니다.\n",
    "df = pd.read_csv('../data/dev.csv')\n",
    "\n",
    "# 'target' column의 범위를 나누어서 해당 범위에 속하는 row의 개수를 카운트합니다.\n",
    "bins = [-0.001, 1.0, 2.0, 3.0, 4.0, 5.0]\n",
    "labels = ['(-0.001, 1.0]', '(1.0, 2.0]', '(2.0, 3.0]', '(3.0, 4.0]', '(4.0, 5.0]']\n",
    "df['target_range'] = pd.cut(df['label'], bins=bins, labels=labels)\n",
    "count = df['target_range'].value_counts()\n",
    "\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0    2140\n",
      "4.0     538\n",
      "0.4     498\n",
      "3.8     474\n",
      "4.2     451\n",
      "3.6     419\n",
      "0.6     392\n",
      "0.8     380\n",
      "1.0     364\n",
      "1.2     359\n",
      "0.2     352\n",
      "3.0     305\n",
      "3.4     301\n",
      "3.2     292\n",
      "2.8     289\n",
      "1.4     271\n",
      "1.8     268\n",
      "2.0     258\n",
      "2.6     257\n",
      "4.4     228\n",
      "2.2     213\n",
      "2.4     208\n",
      "1.6     190\n",
      "5.0     113\n",
      "4.6      96\n",
      "4.8      81\n",
      "0.5      37\n",
      "3.5      34\n",
      "1.5      26\n",
      "2.5      22\n",
      "4.5      18\n",
      "Name: label, dtype: int64\n",
      "0.0    0.216731\n",
      "4.0    0.054487\n",
      "0.4    0.050435\n",
      "3.8    0.048005\n",
      "4.2    0.045676\n",
      "3.6    0.042435\n",
      "0.6    0.039700\n",
      "0.8    0.038485\n",
      "1.0    0.036864\n",
      "1.2    0.036358\n",
      "0.2    0.035649\n",
      "3.0    0.030889\n",
      "3.4    0.030484\n",
      "3.2    0.029573\n",
      "2.8    0.029269\n",
      "1.4    0.027446\n",
      "1.8    0.027142\n",
      "2.0    0.026129\n",
      "2.6    0.026028\n",
      "4.4    0.023091\n",
      "2.2    0.021572\n",
      "2.4    0.021065\n",
      "1.6    0.019242\n",
      "5.0    0.011444\n",
      "4.6    0.009723\n",
      "4.8    0.008203\n",
      "0.5    0.003747\n",
      "3.5    0.003443\n",
      "1.5    0.002633\n",
      "2.5    0.002228\n",
      "4.5    0.001823\n",
      "Name: label, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "label_counts = data_df['label'].value_counts()\n",
    "label_ratios = label_counts / label_counts.sum()\n",
    "print(label_counts)\n",
    "print(label_ratios)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3,
     "status": "ok",
     "timestamp": 1679277876065,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "Roos1YY66VdY",
    "outputId": "0624f94d-7674-4fc3-c713-ae6e6b592147",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             label  binary-label\n",
      "count  9324.000000   9324.000000\n",
      "mean      1.849968      0.389532\n",
      "std       1.602212      0.487670\n",
      "min       0.000000      0.000000\n",
      "25%       0.200000      0.000000\n",
      "50%       1.600000      0.000000\n",
      "75%       3.400000      1.000000\n",
      "max       5.000000      1.000000\n",
      "            label  binary-label\n",
      "count  550.000000    550.000000\n",
      "mean     2.584000      0.520000\n",
      "std      1.459483      0.500055\n",
      "min      0.000000      0.000000\n",
      "25%      1.400000      0.000000\n",
      "50%      2.600000      1.000000\n",
      "75%      3.800000      1.000000\n",
      "max      5.000000      1.000000\n"
     ]
    }
   ],
   "source": [
    "# label과 binary label 통계 분석\n",
    "print(train_df.describe())\n",
    "print(dev_df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 581
    },
    "executionInfo": {
     "elapsed": 1000,
     "status": "ok",
     "timestamp": 1679277875690,
     "user": {
      "displayName": "정지수",
      "userId": "16416363592242911237"
     },
     "user_tz": -540
    },
    "id": "GBXZCBVwtJF4",
    "outputId": "5c83ec4b-3cee-4972-df88-3df206f8860e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# label 분포 확인\n",
    "print(\"train\")\n",
    "draw_box_plot(train_df)\n",
    "\n",
    "print(\"test\")\n",
    "draw_box_plot(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "Sya1yK8v9TCG"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading (…)okenizer_config.json: 100%|██████████| 375/375 [00:00<00:00, 75.0kB/s]\n",
      "c:\\Anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\MuHyeonSon\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Downloading (…)solve/main/vocab.txt: 100%|██████████| 248k/248k [00:00<00:00, 1.35MB/s]\n",
      "Downloading (…)/main/tokenizer.json: 100%|██████████| 752k/752k [00:00<00:00, 1.04MB/s]\n",
      "Downloading (…)cial_tokens_map.json: 100%|██████████| 173/173 [00:00<00:00, 43.4kB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              1_len         2_len         1_unk         2_unk\n",
      "count  11668.000000  11668.000000  11668.000000  11668.000000\n",
      "mean      20.076534     19.556222      0.011827      0.009256\n",
      "std        8.831861      8.296274      0.112769      0.100994\n",
      "min        5.000000      5.000000      0.000000      0.000000\n",
      "25%       14.000000     14.000000      0.000000      0.000000\n",
      "50%       17.000000     17.000000      0.000000      0.000000\n",
      "75%       22.000000     22.000000      0.000000      0.000000\n",
      "max       81.000000     70.000000      2.000000      2.000000\n",
      "test\n",
      "            1_len       2_len       1_unk       2_unk\n",
      "count  519.000000  519.000000  519.000000  519.000000\n",
      "mean    19.473988   19.416185    0.005780    0.005780\n",
      "std      8.378751    8.170257    0.075882    0.075882\n",
      "min      6.000000    7.000000    0.000000    0.000000\n",
      "25%     14.500000   14.000000    0.000000    0.000000\n",
      "50%     17.000000   17.000000    0.000000    0.000000\n",
      "75%     21.000000   22.000000    0.000000    0.000000\n",
      "max     61.000000   58.000000    1.000000    1.000000\n"
     ]
    }
   ],
   "source": [
    "# 토크나이징 결과 분석\n",
    "print(\"train\")\n",
    "tokenizing(train_df)\n",
    "\n",
    "print(\"test\")\n",
    "tokenizing(dev_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "XNZc1fn_9IdV",
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train source unique : ['nsmc-rtt', 'nsmc-sampled', 'petition-rtt', 'petition-sampled', 'slack-rtt', 'slack-sampled']\n",
      "dev source unique : ['nsmc-rtt', 'nsmc-sampled', 'petition-rtt', 'petition-sampled', 'slack-rtt', 'slack-sampled']\n"
     ]
    }
   ],
   "source": [
    "# source 종류 파악\n",
    "print(f\"train source unique : {sorted(train_df['source'].unique())}\")\n",
    "print(f\"dev source unique : {sorted(dev_df['source'].unique())}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rtt : 실제 데이터\n",
    "sampled : 제작데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "kNPx4rp-9KUR",
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'binary_label'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:3800\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3799\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx:138\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/_libs/index.pyx:165\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5745\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:5753\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'binary_label'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdraw_graph\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_df\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[8], line 5\u001b[0m, in \u001b[0;36mdraw_graph\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m      3\u001b[0m source_binary \u001b[38;5;241m=\u001b[39m {source:[\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m source \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28msorted\u001b[39m(df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39munique())}\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, item \u001b[38;5;129;01min\u001b[39;00m df\u001b[38;5;241m.\u001b[39miterrows():\n\u001b[0;32m----> 5\u001b[0m     source_binary[item[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msource\u001b[39m\u001b[38;5;124m'\u001b[39m]][\u001b[43mitem\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mbinary_label\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m] \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m      6\u001b[0m source_binary_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(source_binary)\u001b[38;5;241m.\u001b[39mtranspose()\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# source의 분포\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py:982\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    979\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[1;32m    981\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[0;32m--> 982\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[1;32m    985\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[1;32m    986\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    987\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/series.py:1092\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[1;32m   1091\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[0;32m-> 1092\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1093\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/pandas/core/indexes/base.py:3802\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   3800\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine\u001b[38;5;241m.\u001b[39mget_loc(casted_key)\n\u001b[1;32m   3801\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[0;32m-> 3802\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3803\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3804\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3805\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3806\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'binary_label'"
     ]
    }
   ],
   "source": [
    "draw_graph(train_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UUmmPE3_BHSH"
   },
   "outputs": [],
   "source": [
    "draw_graph(dev_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiQtI728hglL"
   },
   "source": [
    "###**콘텐츠 라이선스**\n",
    "\n",
    "<font color='red'><b>**WARNING**</b></font> : **본 교육 콘텐츠의 지식재산권은 재단법인 네이버커넥트에 귀속됩니다. 본 콘텐츠를 어떠한 경로로든 외부로 유출 및 수정하는 행위를 엄격히 금합니다.** 다만, 비영리적 교육 및 연구활동에 한정되어 사용할 수 있으나 재단의 허락을 받아야 합니다. 이를 위반하는 경우, 관련 법률에 따라 책임을 질 수 있습니다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
